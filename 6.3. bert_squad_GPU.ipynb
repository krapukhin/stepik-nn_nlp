{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT для вопросно-ответных систем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Научим BERT отвечать на вопросы (question-answering). Стандартный датасет для тренировки QA моделей (или question-answering моделей) называется SQUAD[2] и расшифровывается как Stanford Question Answering Dataset. Он включает в себя вопросы и ответы на них по достаточно разнообразной тематике. Вопросы формировались на основе статей или отрывков из статьи из Википедии. Ответ на каждый вопрос представляет собой сегмент текста или промежуток из соответствующего отрывка. В новой версии датасета (в 2.0 версии) даже возможны вопросы без ответа. Имеется в виду, что есть вопросы, для ответа на которые недостаточно информации в предложенном фрагменте текста. Таким образом, алгоритм составления этого датасета был следующим. Асессор читал фрагмент текста из Википедии (как правило, всего несколько абзацев), затем формулировал вопрос по прочитанному и фиксировал правильный ответ. На большинство вопросов присутствует несколько вариантов ответа. Но, как правило, это всего лишь переформулировки одного и того же, по смыслу, варианта ответа. Например, на вопрос \"когда началась эпоха Возрождения в Италии\" можно ответить \"14 век\", \"в 14 веке\", \"в начале 14 века\" или как-то ещё, использовав слова \"14\" и \"век\". Учитывая специфику нашего датасета, на вход нейросети мы будем подавать не только сам вопрос, но и соответствующий фрагмент текста, параграф текста. А в качестве выхода нейросети будем ожидать две позиции в тексте: начало ответа и конец ответа. Два файла, один — с обучающей выборкой (train.json), и с выборкой, на которой мы будем тестировать наш алгоритм (он называется dev_version2.json). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "# import sys; sys.path.append('./stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте датасет (SQuAD) [отсюда](https://rajpurkar.github.io/SQuAD-explorer/). Для выполенения семинара Вам понадобятся файлы `train-v2.0.json` и `dev-v2.0.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Склонируйте репозиторий https://github.com/huggingface/transformers (воспользуйтесь скриптом `clone_pytorch_transformers.sh`) и положите путь до папки `examples` в переменную `PATH_TO_EXAMPLES`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, давайте сначала склонируем (clone) репозитории и положим путь до папки \"examples\" в переменную path_to_examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRANSFORMERS_REPO = './datasets/transformers/'\n",
    "import os\n",
    "os.environ['PATH_TO_TRANSFORMER_REPO'] = PATH_TO_TRANSFORMERS_REPO\n",
    "# ! bash clone_pytorch_transformers.sh $PATH_TO_TRANSFORMERS_REPO\n",
    "import sys\n",
    "\n",
    "PATH_TO_EXAMPLES = os.path.join(PATH_TO_TRANSFORMERS_REPO, 'examples')\n",
    "sys.path.append(PATH_TO_EXAMPLES)\n",
    "import torch\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "from utils_squad import (read_squad_examples, convert_examples_to_features,\n",
    "                         RawResult, write_predictions,\n",
    "                         RawResultExtended, write_predictions_extended)\n",
    "\n",
    "from run_squad import train, load_and_cache_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "\n",
    "from transformers import (WEIGHTS_NAME, BertConfig, XLNetConfig, XLMConfig,\n",
    "                          BertForQuestionAnswering, BertTokenizer)\n",
    "\n",
    "from utils_squad_evaluate import EVAL_OPTS, main as evaluate_on_squad\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше, так же как и в прошлом семинаре, загружаем токенизатор для BERT и готовую модификацию BERT для вопросно-ответных систем под названием \"bert for question answering\" из библиотеки pytorch-transformers. Выполняем вот этот кусочек кода. Загрузка модели может занять какое-то время. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, модель скачалась. Теперь у вас есть два варианта развития событий — простой и чуть-чуть более сложный. Вы можете попробовать дообучить модель на датасете SQUAD[2] самостоятельно или же загрузить уже предобученные веса сети и перейти сразу к блоку \"оценка качества работы модели\".[1] Подгрузить веса нейросети можно с помощью функции \"load_state_dict\", как вы уже помните. И при подгрузке весов в случае отсутствия GPU не забудьте указать параметр \"map_location\" (вот как в этой строке кода). [1] https://stepik.org/lesson/268748/step/7?unit=249768 [2] https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы хотим натренировать сеть отвечать на вопросы по статьям с Википедии. Будем использовать для обучения датасет SQuAD (Stanford Question Answering Dataset). Какую информацию нужно подавать на вход нейросети во время обучения?\n",
    "\n",
    "-Несколько произвольных параграфов текста, не имеющих отношения к вопросу  \n",
    "\n",
    "+Правильный ответ на вопрос  \n",
    "\n",
    "+Вопрос  \n",
    "\n",
    "+Параграф текста, по которому задается вопрос  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если Вы не хотите запускать файн-тюнинг, пропустите блок \"Дообучение\",\n",
    "# подгрузите веса уже дообученной модели и переходите к блоку \"Оценка качества\"\n",
    "\n",
    "# скачайте веса с Google-диска и положите их в папку models\n",
    "# https://drive.google.com/drive/folders/1-DR30q7MF-gZ51TDx596dAOhgh-uOAPj?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load('models/bert_squad_1epoch.pt')) # если у вас есть GPU\n",
    "else:\n",
    "    model.load_state_dict(torch.load('models/bert_squad_1epoch.pt', map_location=device)) # если GPU нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, рассмотрим процесс дообучения модели. Аналогичный результат можно получить, просто запустив скрипт run_squad.py[1] из папки \"examples\". Первый параметр называется \"train_file\" — сюда мы кладём путь до обучающей выборки, в переменную \"predict_file\", соответственно, кладём путь до датасета, на котором мы будем тестироваться и оценивать качество работы модели. Ещё из интересных параметров — у нас есть параметр \"model_type\", здесь нужно указать, с какой моделью мы будем работать (в нашем случае, это BERT, но также можно было указать, например, XLNet на DistilBERT). А в параметр \"model_name_or_path\" мы кладём либо путь до предобученной модели, если таковая у вас есть, либо же название модели из переменной \"all_models\". Эту переменную \"all_models\" я определила после этого класса — вот она, давайте посмотрим, что в ней содержится. В этой переменной есть все варианты моделей, с которыми мы можем работать с помощью данного кода (с помощью кодов библиотеки pytorch-transformers). Если говорить про BERT, то у нас есть достаточно вариантов — есть варианты моделей BERT Base и BERT Large, можно использовать \"uncased\" или \"cased\" варианты, можно брать multilingual-модели, либо же какие-то специально заточенные под конкретный язык (например, BERT для китайского языка). Мы же будем использовать самый простой вариант \"bert-base-uncased\". Давайте продолжим наше путешествие по параметрам, которые мы будем использовать для обучения модели. Ещё из интересных параметров стоит отметить \"version_2_with_negative\" — этот параметр отвечает за то, будем ли мы при обучении учитывать вопросы, на которые ответа в датасете нет. Также мы можем указать максимальную длину параграфа, который мы подаём в нашу нейросеть (в нашем случае это будет 384, имеется виду 384 токена, а не 384 символа). Также можно указать страйд — параметр \"doc_stride\" отвечает за то, сколько страйда мы будем использовать при делении длинного документа на чанки (chunks), на небольшие кусочки. Кроме того, можно указать максимальное количество токенов в вопросе — за это отвечает параметр \"max_query_length\" (в нашем случае, это 128) и, также, можно указать максимальную длину ответа — это параметр \"max_answer_length\". Кроме того, давайте посмотрим на параметры, которые нам нужны непосредственно для обучения нейросети. А именно, мы указываем \"learning_rate\", указываем \"weight_decay\" (в нашем случае, мы его не используем, он равен нулю). Можем указать эпсилон для оптимизатора Adam, также мы, конечно, указываем, сколько эпох мы будем тренировать нашу модель. В нашем случае это всего 5 эпох. Кроме того, мы указываем, сколько у нас будет warmup-шагов (в нашем случае мы вовсе не будем использовать warm-up), ещё из интересных параметров стоит отметить вот эти две вещи — \"logging_steps\" и \"save_steps\". Мы говорим, через сколько шагов мы хотим логировать (log) прогресс нашей модели и сохранять чекпойнты (checkpoints). В нашем случае — будем сохранять чекпойнты каждые 5 000 шагов. Также, если у нас выставлен параметр \"evaluate_during_training\", через каждые 5 000 шагов наша модель будет оценивать качество своей работы на датасете из predict_file (на датасете, который лежит вот по этому пути). И также наша модель будет выводить нам список метрик, которые удалось достичь на текущий момент тренировки. Кроме того, мы можем указать — печатать ли warnings, можем указать, что мы не хотим использовать CUDA (даже если у нас доступна GPU). Кроме того, можем задать размеры батча для тренировки и для evaluation. [1] https://github.com/huggingface/transformers/blob/master/examples/run_squad.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dataclasses\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TRAIN_OPTS:\n",
    "    train_file : str = 'train-v2.0.json'    # SQuAD json-файл для обучения\n",
    "    predict_file : str = 'dev-v2.0.json'    # SQuAD json-файл для тестирования\n",
    "    model_type : str = 'bert'               # тип модели (может быть  'bert', 'xlnet', 'xlm', 'distilbert')\n",
    "    model_name_or_path : str = 'bert-base-uncased' # путь до предобученной модели или название модели из ALL_MODELS\n",
    "    output_dir : str = '/tmp' # путь до директории, где будут храниться чекпоинты и предсказания модели\n",
    "    device : str = 'cuda' # cuda или cpu\n",
    "    n_gpu : int = 1 # количество gpu для обучения\n",
    "    cache_dir : str = '' # где хранить предобученные модели, загруженные с s3\n",
    "        \n",
    "    # Если true, то в датасет будут включены вопросы, на которые нет ответов.\n",
    "    version_2_with_negative : bool = True\n",
    "    # Если (null_score - best_non_null) больше, чем порог, предсказывать null.\n",
    "    null_score_diff_threshold : float = 0.0\n",
    "    # Максимальная длина входной последовательности после WordPiece токенизации. Sequences \n",
    "    # Последовательности длиннее будут укорочены, для более коротких последовательностей будет использован паддинг\n",
    "    max_seq_length : int = 384\n",
    "    # Сколько stride использовать при делении длинного документа на чанки\n",
    "    doc_stride : int = 128\n",
    "    # Максимальное количество токенов в вопросе. Более длинные вопросы будут укорочены до этой длины\n",
    "    max_query_length : int = 128 #\n",
    "        \n",
    "    do_train : bool = True\n",
    "    do_eval : bool = True\n",
    "        \n",
    "    # Запускать ли evaluation на каждом logging_step\n",
    "    evaluate_during_training : bool = True\n",
    "    # Должно быть True, если Вы используете uncased модели\n",
    "    do_lower_case : bool = True #\n",
    "    \n",
    "    per_gpu_train_batch_size : int = 8 # размер батча для обучения\n",
    "    per_gpu_eval_batch_size : int = 8 # размер батча для eval\n",
    "    learning_rate : float = 5e-5 # learning rate\n",
    "    gradient_accumulation_steps : int = 1 # количество шагов, которые нужно сделать перед backward/update pass\n",
    "    weight_decay : float = 0.0 # weight decay\n",
    "    adam_epsilon : float = 1e-8 # эпсилон для Adam\n",
    "    max_grad_norm : float = 1.0 # максимальная норма градиента\n",
    "    num_train_epochs : float = 5.0 # количество эпох на обучение\n",
    "    max_steps : int = -1 # общее количество шагов на обучение (override num_train_epochs)\n",
    "    warmup_steps : int = 0 # warmup \n",
    "    n_best_size : int = 5 # количество ответов, которые надо сгенерировать для записи в nbest_predictions.json\n",
    "    max_answer_length : int = 30 # максимально возможная длина ответа\n",
    "    verbose_logging : bool = True # печатать или нет warnings, относящиеся к обработке данных\n",
    "    logging_steps : int = 5000 # логировать каждые X шагов\n",
    "    save_steps : int = 5000 # сохранять чекпоинт каждые X шагов\n",
    "        \n",
    "    # Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\n",
    "    eval_all_checkpoints : bool = True\n",
    "    no_cuda : bool = False # не использовать CUDA\n",
    "    overwrite_output_dir : bool = True # переписывать ли содержимое директории с выходными файлами\n",
    "    overwrite_cache : bool = True # переписывать ли закешированные данные для обучения и evaluation\n",
    "    seed : int = 42 # random seed\n",
    "    local_rank : int = -1 # local rank для распределенного обучения на GPU\n",
    "    fp16 : bool = False # использовать ли 16-bit (mixed) precision (через NVIDIA apex) вместо 32-bit\"\n",
    "    # Apex AMP optimization level: ['O0', 'O1', 'O2', and 'O3'].\n",
    "    # Подробнее тут: https://nvidia.github.io/apex/amp.html\n",
    "    fp16_opt_level : str = '01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) \\\n",
    "                  for conf in (BertConfig, XLNetConfig, XLMConfig)), ())\n",
    "ALL_MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " мы рассмотрели основные параметры, которые нам нужны для тренировки модели, теперь можем подгрузить наши данные и запустить тренировку. Давайте загрузим наш датасет. Обратите внимание на параметр \"evaluate\" в этой строчке кода. Если здесь будет написано \"evaluate = True\", то никакого обучения происходить не будет. Запустится evaluation на наших данных, а веса модели меняться никак не будут. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = TRAIN_OPTS()\n",
    "train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того, как мы загрузили наш датасет (это достаточно долгий процесс...), мы можем запустить обучение нашей модели (с помощью вот этой строчки кода). Полное обучение всех пяти эпох, как я уже говорила, заняло около 7 часов, поэтому запускать эту ячейку мы не будем. Отмечу лишь, что через каждые 5 000 шагов, как и обсуждалось ранее, у нас происходит evaluation модели. В нашем случае, это примерно через каждые 30 процентов работы алгоритма. То есть, на 5 000 итерации из 16500 (примерно) мы запускаем evaluation. Evaluation тоже занимает некоторое время, но зато в конце мы можем посмотреть на метрики, которые у нас получились. Например, после первых 5 000 итерации мы видим, что наша модель дала абсолютно правильный ответ в 57% случаев, f-мера составила 69.16 (примерно). В принципе, довольно неплохо. Но, тем не менее, давайте \"дотренируем\" нашу модель дальше (точнее — посмотрим, как она тренировалась). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(args, train_dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем веса дообученной модели на диск, чтобы в следующий раз не обучать модель заново."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И после того как наша модель обучилась мы можем сохранить веса нашей модели (например, в файл \"bert_squad[1]_final_5epochs\"). А потом, если захотим, мы сможем подгрузить веса нашей модели с помощью вот этой строчки кода, которую мы уже видели чуть ранее. [1] https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/bert_squad_final_5epoch.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузить веса модели можно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/bert_squad_5epochs.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сопоставьте названия и определения метрик, которые измерялись в семинаре для оценки качества работы модели.  \n",
    "- HasAns_exact - Процент полностью совпавших с ground truth ответов на вопросы, на которые в датасете есть ответ  \n",
    "- NoAns_exact - Процент полностью совпавших с ground truth ответов на вопросы, на которые в датасете нет ответов  \n",
    "- f1 - F1-мера (среднее геометрическое между precision и recall), измеряющая среднее перекрытие между предсказанным вариантом ответа и ground truth ответом  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка качества работы модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И хотя, в процессе обучения, после каждых 5 000 итераций, модель производила evaluation на dev-датасете и выводила нам метрики, давайте всё-таки сделаем evaluation ещё раз. Чтобы не просматривать слишком много вопросов вручную, давайте отделим маленький кусочек, состоящий из всего 208 вопросов, и оценим качество работы модели на нём. Также наши метрики будут считаться быстрее на этом маленьком кусочке, чем на всём dev-датасете. Отделим наш маленький датасет с помощью следующего кода, считываем наш dev.json файл, отделяем 208 вопросов и записываем его в файл small_dev.json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DEV_SQUAD = 'dev-v2.0.json'\n",
    "PATH_TO_SMALL_DEV_SQUAD = 'small_dev-v2.0.json'\n",
    "\n",
    "with open(PATH_TO_DEV_SQUAD, 'r') as iofile:\n",
    "    full_sample = json.load(iofile)\n",
    "    \n",
    "small_sample = {\n",
    "    'version': full_sample['version'],\n",
    "    'data': full_sample['data'][:1]\n",
    "}\n",
    "\n",
    "with open(PATH_TO_SMALL_DEV_SQUAD, 'w') as iofile:\n",
    "    json.dump(small_sample, iofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь объявим несколько констант, которые нам будут нужны для evaluation. Определяем максимальную длину параграфа (384 — так же как и для обучения), также определяем максимальную длину вопроса, максимальную длину ответа. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 384\n",
    "outside_pos = max_seq_length + 10\n",
    "doc_stride = 128\n",
    "max_query_length = 64\n",
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь, с помощью функции \"read_squad[1]_examples()\", мы загрузим наши 208 вопросов и будем дальше с ними работать (загружаем наши вопросы с помощью вот этого кода). Здесь мы выставляем \"is_training = False\", потому что мы собираемся только оценить качество работы нашей модели и ничего обучать мы здесь не будем. И дальше, с помощью функции \"convert_examples_to_features()\" превращаем загруженные данные в фичи. Здесь мы ставим параметр \"is_training\" в значении False, так же как и для функции read_examples, и используем тот же самый токенайзер, что и при обучении (вот этот токенайзер). Также передаём наши примеры, загруженные с помощью \"read_squad[1]_examples()\". После этого вытаскиваем из наших фичей \"input_ids\", \"input_mask\", \"segment_ids\", \"cls_index\" и \"p_mask\". Формат, на самом деле, очень похож на тот, что мы обсуждали в прошлом семинаре, когда работали с BERT для классификации предложений. \"input_ids\" — это просто последовательность чисел, отождествляющих каждый токен с его номером в словаре (также, как и в прошлом семинаре). \"input_mask\" — это последовательность из нулей и единиц, где единицы обозначают токены предложения, а нули — паддинг. Похоже на \"attention_mask\" из ноутбука про классификацию с помощью BERT на прошлом семинаре. Переменная \"p_mask\" означает следующее — здесь мы будем маскировать единицами токены, которых не может быть в ответе, а нулями — все токены, которые могут встретиться в нашем ответе. Ну, и \"cls_index\" — это индекс нашего классификационного токена. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "examples = read_squad_examples(\n",
    "    input_file=PATH_TO_SMALL_DEV_SQUAD,\n",
    "    is_training=False,\n",
    "    version_2_with_negative=True)\n",
    "\n",
    "features = convert_examples_to_features(\n",
    "    examples=examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    doc_stride=doc_stride,\n",
    "    max_query_length=max_query_length,\n",
    "    is_training=False,\n",
    "    cls_token_segment_id=0,\n",
    "    pad_token_segment_id=0,\n",
    "    cls_token_at_end=False\n",
    ")\n",
    "\n",
    "input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n",
    "p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n",
    "\n",
    "example_index = torch.arange(input_ids.size(0), dtype=torch.long)\n",
    "dataset = TensorDataset(input_ids, input_mask, segment_ids, example_index, cls_index, p_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше мы передаём все эти переменные в тензор Dataset (вот здесь) и, дальше, создаём из него DataLoader (вот таким образом). Здесь мы передаём sampler и определяем размер батча. В нашем случае, размер батча будет равен 8. [1] https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sampler = SequentialSampler(dataset)\n",
    "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте запустим evaluation модели на нашем маленьком кусочке dev-датасета. Evaluation может занять достаточно большое время. В нашем случае это будет не больше пары минут, и, пока, давайте рассмотрим код, который у нас есть для evaluation модели. Здесь мы используем наш \"eval_dataloader\", мы разбиваем его на батчи. В цикле проходимся по нашим батчам (размер каждого батча равен 8). Дальше задаём, что мы не хотим считать градиент на каждом шаге, поскольку у нас происходит процесс evaluation, а не обучения модели. Мы распаковываем наш батч и передаём полученные данные в нашу модель, чтобы получить предсказание модели. И дальше складываем полученные результаты в list под названием \"all_results\". Давайте посмотрим на то, как выглядит наш лист all_results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    "\n",
    "all_results = []\n",
    "for idx, batch in enumerate(tqdm.tqdm_notebook(eval_dataloader, desc=\"Evaluating\")):\n",
    "    model.eval()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1]\n",
    "                  }\n",
    "        inputs['token_type_ids'] = batch[2]\n",
    "        example_indices = batch[3]\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    for i, example_index in enumerate(example_indices):\n",
    "        eval_feature = features[example_index.item()]\n",
    "        unique_id = int(eval_feature.unique_id)\n",
    "        result = RawResult(unique_id    = unique_id,\n",
    "                           start_logits = to_list(outputs[0][i]),\n",
    "                           end_logits   = to_list(outputs[1][i]))\n",
    "        all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, эта ячейка кода выполнилась — смотрим на то что у нас содержится в листе all_results. На самом деле, выход не очень интерпретируемый. У нас есть некоторые айдишники, у нас есть start_logits, есть end_logits, и в целом, по вот такому вот списку достаточно сложно что-то понять. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому давайте, с помощью функции \"write_predictions()\", которая уже есть в готовом виде в библиотеке pytorch-transformers, сформируем человеко-читаемый ответ. Для начала, нам нужно задать несколько констант. Давайте зададим параметры \"n_best_size\" равным 5 — это значит, что на каждый вопрос мы будем генерировать по 5 самых лучших ответов. Также нам нужно задать пути до файла с нашими предсказаниями, в файл под названием \"output_1_best_file\" мы будем писать айдишник нашего вопроса и один наиболее вероятный ответ на этот вопрос. Этот файл нам нужен для того, чтобы посчитать метрики, потому что для подсчёта метрик нам нужно не 5 возможных вариантов ответа, а всего один наиболее вероятный. Для того чтобы просмотреть глазами наши варианты ответа, мы всё-таки хотим видеть не один вариант, в котором наиболее уверена наша модель, а несколько вариантов. Поэтому будем записывать N лучших вариантов (в нашем случае, 5 лучших вариантов) в файл \"output_n_best_file\". И, кроме того, давайте поставим, что параметр \"version_2_with_negative\" у нас будет равен True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 5\n",
    "do_lower_case = True\n",
    "output_prediction_file = 'output_1best_file'\n",
    "output_nbest_file = 'output_nbest_file'\n",
    "output_na_prob_file = 'output_na_prob_file'\n",
    "verbose_logging = True\n",
    "version_2_with_negative = True\n",
    "null_score_diff_threshold = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируем предсказания с помощью функции \"write_predictions()\". Файл с предсказанием будет выглядеть следующим образом. У нас есть ordered dict, в котором содержится ID вопроса и наиболее вероятный вариант ответа на него. И, как раз, вся эта информация лежит в файле \"output_1_best_file\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем файл с n лучшими ответами `output_nbest_file`\n",
    "write_predictions(examples, features, all_results, n_best_size,\n",
    "                    max_answer_length, do_lower_case, output_prediction_file,\n",
    "                    output_nbest_file, output_na_prob_file, verbose_logging,\n",
    "                    version_2_with_negative, null_score_diff_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте посчитаем метрики на нашем датасете. Мы будем использовать \"eval_opts\" — по структуре оно очень похоже на класс \"train_opts\", который мы использовали, только поля чуть-чуть другие. И, кроме того, с помощью функции и \"evaluate_on_squad[1]()\" мы можем посчитать метрики, используя наши evaluate options. Давайте посчитаем их! Получается, что абсолютно правильный ответ, с точностью до символа, был получен в 78% случаев — это достаточно неплохо. f-мера 68, а если говорить про вопросы, на которые нет ответа, то BERT смог выдать правильный ответ, то есть угадать, что на этот вопрос нет ответа, в 54% случаев. [1] https://rajpurkar.github.io/SQuAD-explorer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считаем метрики используя официальный SQuAD script\n",
    "evaluate_options = EVAL_OPTS(data_file=PATH_TO_SMALL_DEV_SQUAD,\n",
    "                             pred_file=output_prediction_file,\n",
    "                             na_prob_file=output_na_prob_file)\n",
    "results = evaluate_on_squad(evaluate_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим глазами на вопросы и предсказанные БЕРТом ответы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы смогли получить достаточно высокие значения метрик, но всё-таки давайте посмотрим глазами на вопросы и предсказанные BERT ответы, а также на каноничные ответы из нашего датасета. Давайте прочитаем файл с названием \"output_n_best_file\". Дальше сформируем словарь, в котором хранятся ID вопроса, а также текст вопроса, варианты ответа из нашего датасета и параграф, на котором основывался вопрос. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_nbest_file', 'r') as iofile:\n",
    "    predicted_answers = json.load(iofile)\n",
    "questions = {}\n",
    "\n",
    "for paragraph in small_sample['data'][0]['paragraphs']:\n",
    "    for question in paragraph['qas']:\n",
    "        questions[question['id']] = {\n",
    "            'question': question['question'],\n",
    "            'answers': question['answers'],\n",
    "            'paragraph': paragraph['context']\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И распечатаем вопросы-ответы BERT и ответы из нашего датасета. Для простоты понимания, давайте параграфы пока печатать не будем. Как вы видите, BERT с достаточно хорошей уверенностью (со стопроцентной уверенностью) отвечает правильно на первый вопрос. Похожая история происходит со вторым вопросом, BERT отвечает правильно с точностью до артикля, при этом, уверенность 71%. На третий вопрос мы также получаем правильный ответ с уверенностью 100%, и похожая ситуация происходит с остальными вопросами. Если говорить про вопросы, на которые в датасете ответа нет, то BERT достаточно неплохо предсказывает отсутствие ответа на вопрос, то есть BERT, примерно в 50% случаев, научился понимать, что информации, содержащаяся в параграфе, недостаточно для ответа на этот вопрос. Всего здесь 208 вопросов — вы можете более подробно изучить ответы BERT на них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for q_num, (key, data) in enumerate(predicted_answers.items()):\n",
    "    gt = '' if len(questions[key]['answers']) == 0 else questions[key]['answers'][0]['text']\n",
    "    print('Вопрос {0}:'.format(q_num+1), questions[key]['question'])\n",
    "    print('-----------------------------------')\n",
    "    print('Ground truth:', gt)\n",
    "    print('-----------------------------------')   \n",
    "    print('Ответы, предсказанные БЕРТом:')\n",
    "    preds = ['{0}) '.format(ans_num + 1) + answer['text'] + \\\n",
    "             ' (уверенность {0:.2f}%)'.format(answer['probability']*100) \\\n",
    "             for ans_num, answer in enumerate(data)]\n",
    "    print('\\n'.join(preds))\n",
    "#     print('-----------------------------------')   \n",
    "#     print('Параграф:', questions[key]['paragraph'])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтоб получить такие достаточно хорошие результаты, мы тренировали BERT в целых пять эпох. Что же будет, если мы потренируем BERT чуть меньшее количество времени? Как вы помните, тренировать 5 эпох BERT заняло достаточно много времени. Давайте попробуем потренировать всего одну эпоху и посмотрим, что произойдет, насколько метрики станут ниже. Мы не будем тренировать нашу модель прямо сейчас, а просто подгрузим веса модели после тренировки одной эпохи (файлик называется \"bert_squad[1]_1epoch\"[1] https://rajpurkar.github.io/SQuAD-explorer/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгружаем веса и ещё раз делаем evaluation. Посмотрим на метрики. Как вы видите, метрики стали чуть ниже, но не сильно ниже. На вопросы с ответом модель даёт правильный ответ в 76% случаев, сравнивая с 78% после 5 эпох, а f-мера стала равна 71. Кроме того, на вопросы без ответа — на вопросы, на которые нельзя ответить, учитывая данный параграф текста, модель научилась отвечать (точнее... не отвечать) даже лучше: 61% случаев, сравнивая с 54% после пяти эпох. Получается, что если вам нужен сравнительно быстрый результат, можно немного пожертвовать качеством работы модели и обучать модель не 7 часов, а примерно полтора часа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом семинаре мы разобрали, как работает скрипт по дообучению модели BertForQuestionAnswering из библиотеки pytorch-transformers. Получить аналогичные результаты, на самом деле, вы могли бы просто склонировав репозитории и запустив скрипт \"run_squad.py\"[1] (например, вот так — вот таким образом). Однако, если вам вдруг понадобится что-то поменять в скрипте, модифицировать процесс обучения или даже просто обучить модель на другом, чуть менее популярном датасете, вам придётся закапываться в код библиотеки гораздо глубже, чем простой вызов готового скрипта. Этот семинар может послужить отправной точкой в более подробном освоении кода библиотеки и запуске BERT, XL трансформера, GPT-2 или любых других моделей для решения ваших задач. Успехов!\n",
    "[1] https://github.com/huggingface/transformers/blob/master/examples/run_squad.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export SQUAD_DIR=/path/to/SQUAD\n",
    "\n",
    "python run_squad.py \\\n",
    "  --model_type bert \\\n",
    "  --model_name_or_path bert-base-cased \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --train_file $SQUAD_DIR/train-v1.1.json \\\n",
    "  --predict_file $SQUAD_DIR/dev-v1.1.json \\\n",
    "  --per_gpu_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2.0 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /tmp/debug_squad/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
